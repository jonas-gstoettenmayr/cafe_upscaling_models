{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34ae95b",
   "metadata": {},
   "source": [
    "# Modelling - Bettina Pölzleitner (Lead) and Jonas Gstöttenmayr (Assistant)\n",
    "Finding the best combination of predictor and dataset features will be determined and validated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f1b99",
   "metadata": {},
   "source": [
    "add to say model_stats/ml/dl is exploratory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0af6fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e9856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "dark_style = {\n",
    "    'figure.facecolor': '#212946',\n",
    "    'axes.facecolor': '#212946',\n",
    "    'savefig.facecolor':'#212946',\n",
    "    'axes.grid': True,\n",
    "    'axes.grid.which': 'both',\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'grid.color': '#2A3459',\n",
    "    'grid.linewidth': '1',\n",
    "    'text.color': '0.9',\n",
    "    'axes.labelcolor': '0.9',\n",
    "    'xtick.color': '0.9',\n",
    "    'ytick.color': '0.9',\n",
    "    'font.size': 12 }\n",
    "plt.rcParams.update(dark_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54af9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/processed_data\")\n",
    "parquet_files = list(data_path.glob(\"*.parquet\"))\n",
    "# dfs = {f.stem: pl.read_parquet(f) for f in parquet_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e13589",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicct = {\"train\": {}, \"val\": {},\"test\": {}, \"future\": {}, \"train_future\": {}, \"val_future\": {},  \"test_future\": {}}\n",
    "mapping = {\"fourier+trend+holidays\": \"fth\", \"fourier\": \"fou\", \"holidays\": \"hol\", \"none\": \"non\", \"trend\": \"tre\"}\n",
    "for f in parquet_files:\n",
    "    name = f.stem\n",
    "    split = name.split(\"_\", 1)\n",
    "    if split[-1] in dicct.keys():\n",
    "        dicct[split[-1]][mapping[split[0]]] = pl.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c81d984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (19_180, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ds</th><th>unique_id</th><th>y</th></tr><tr><td>date</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>2020-07-01</td><td>&quot;Bubble tea&quot;</td><td>2012.0</td></tr><tr><td>2020-07-02</td><td>&quot;Bubble tea&quot;</td><td>2085.0</td></tr><tr><td>2020-07-03</td><td>&quot;Bubble tea&quot;</td><td>2204.0</td></tr><tr><td>2020-07-04</td><td>&quot;Bubble tea&quot;</td><td>2119.0</td></tr><tr><td>2020-07-05</td><td>&quot;Bubble tea&quot;</td><td>2176.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2025-09-26</td><td>&quot;Tea&quot;</td><td>1206.0</td></tr><tr><td>2025-09-27</td><td>&quot;Tea&quot;</td><td>1074.0</td></tr><tr><td>2025-09-28</td><td>&quot;Tea&quot;</td><td>1222.0</td></tr><tr><td>2025-09-29</td><td>&quot;Tea&quot;</td><td>1211.0</td></tr><tr><td>2025-09-30</td><td>&quot;Tea&quot;</td><td>1220.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (19_180, 3)\n",
       "┌────────────┬────────────┬────────┐\n",
       "│ ds         ┆ unique_id  ┆ y      │\n",
       "│ ---        ┆ ---        ┆ ---    │\n",
       "│ date       ┆ str        ┆ f64    │\n",
       "╞════════════╪════════════╪════════╡\n",
       "│ 2020-07-01 ┆ Bubble tea ┆ 2012.0 │\n",
       "│ 2020-07-02 ┆ Bubble tea ┆ 2085.0 │\n",
       "│ 2020-07-03 ┆ Bubble tea ┆ 2204.0 │\n",
       "│ 2020-07-04 ┆ Bubble tea ┆ 2119.0 │\n",
       "│ 2020-07-05 ┆ Bubble tea ┆ 2176.0 │\n",
       "│ …          ┆ …          ┆ …      │\n",
       "│ 2025-09-26 ┆ Tea        ┆ 1206.0 │\n",
       "│ 2025-09-27 ┆ Tea        ┆ 1074.0 │\n",
       "│ 2025-09-28 ┆ Tea        ┆ 1222.0 │\n",
       "│ 2025-09-29 ┆ Tea        ┆ 1211.0 │\n",
       "│ 2025-09-30 ┆ Tea        ┆ 1220.0 │\n",
       "└────────────┴────────────┴────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicct[\"train\"][\"non\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf7ed1",
   "metadata": {},
   "source": [
    "## Statistical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94c6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jojog\\Documents\\School\\Semester_3\\TIS3\\cafe_upscaling_models\\.venv\\Lib\\site-packages\\statsforecast\\models.py:3806: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  \"\"\"CrostonClassic model.\n",
      "c:\\Users\\jojog\\Documents\\School\\Semester_3\\TIS3\\cafe_upscaling_models\\.venv\\Lib\\site-packages\\statsforecast\\models.py:3984: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  \"\"\"CrostonOptimized model.\n",
      "c:\\Users\\jojog\\Documents\\School\\Semester_3\\TIS3\\cafe_upscaling_models\\.venv\\Lib\\site-packages\\statsforecast\\models.py:4132: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  \"\"\"CrostonSBA model.\n",
      "c:\\Users\\jojog\\Documents\\School\\Semester_3\\TIS3\\cafe_upscaling_models\\.venv\\Lib\\site-packages\\statsforecast\\models.py:4468: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  \"\"\"TSB model.\n"
     ]
    }
   ],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoRegressive, SeasonalExponentialSmoothingOptimized, HoltWinters, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "018d6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stats_model(dataset):\n",
    "    season_length = 7 # Monthly data\n",
    "    horizon = int(len(dicct[\"test\"][\"non\"])/10) # number of predictions\n",
    "\n",
    "    models = [AutoARIMA(season_length=season_length),\n",
    "            AutoRegressive(lags=[14], include_mean=True),\n",
    "            SeasonalExponentialSmoothingOptimized(season_length=season_length),\n",
    "            HoltWinters(season_length=season_length, error_type=\"A\", alias=\"Add\")]\n",
    "    \n",
    "    sf = StatsForecast(models=models, freq='D', n_jobs = -1)\n",
    "\n",
    "    sf.fit(df=dicct[\"train\"][dataset])\n",
    "    \n",
    "    preditions = sf.predict(h = horizon)\n",
    "    return preditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8858cc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>AutoARIMA</th>\n",
       "      <th>AutoRegressive</th>\n",
       "      <th>SeasESOpt</th>\n",
       "      <th>Add</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Bubble tea\"</td>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>1235.053173</td>\n",
       "      <td>1603.159410</td>\n",
       "      <td>1201.817308</td>\n",
       "      <td>1178.244696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Bubble tea\"</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>1285.240607</td>\n",
       "      <td>1514.111142</td>\n",
       "      <td>1189.601772</td>\n",
       "      <td>1216.963902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Bubble tea\"</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>1441.943523</td>\n",
       "      <td>1651.168042</td>\n",
       "      <td>1475.429401</td>\n",
       "      <td>1343.298368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bubble tea\"</td>\n",
       "      <td>2025-10-04</td>\n",
       "      <td>1467.632445</td>\n",
       "      <td>1696.079342</td>\n",
       "      <td>1534.828116</td>\n",
       "      <td>1520.046579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Bubble tea\"</td>\n",
       "      <td>2025-10-05</td>\n",
       "      <td>1345.414798</td>\n",
       "      <td>1650.393709</td>\n",
       "      <td>1404.869409</td>\n",
       "      <td>1516.852784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>\"Tea\"</td>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>1214.627571</td>\n",
       "      <td>1436.114396</td>\n",
       "      <td>1240.654246</td>\n",
       "      <td>1204.376328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>\"Tea\"</td>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>1216.719091</td>\n",
       "      <td>1442.204258</td>\n",
       "      <td>1259.278983</td>\n",
       "      <td>1222.421475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>\"Tea\"</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>1214.777405</td>\n",
       "      <td>1530.822126</td>\n",
       "      <td>1240.372584</td>\n",
       "      <td>1238.689590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>\"Tea\"</td>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>1216.580007</td>\n",
       "      <td>1545.850469</td>\n",
       "      <td>1198.768587</td>\n",
       "      <td>1216.639854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>\"Tea\"</td>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>1214.906532</td>\n",
       "      <td>1489.076728</td>\n",
       "      <td>1182.720575</td>\n",
       "      <td>1151.727614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        unique_id         ds    AutoARIMA  AutoRegressive    SeasESOpt  \\\n",
       "0    \"Bubble tea\" 2025-10-01  1235.053173     1603.159410  1201.817308   \n",
       "1    \"Bubble tea\" 2025-10-02  1285.240607     1514.111142  1189.601772   \n",
       "2    \"Bubble tea\" 2025-10-03  1441.943523     1651.168042  1475.429401   \n",
       "3    \"Bubble tea\" 2025-10-04  1467.632445     1696.079342  1534.828116   \n",
       "4    \"Bubble tea\" 2025-10-05  1345.414798     1650.393709  1404.869409   \n",
       "..            ...        ...          ...             ...          ...   \n",
       "305         \"Tea\" 2025-10-27  1214.627571     1436.114396  1240.654246   \n",
       "306         \"Tea\" 2025-10-28  1216.719091     1442.204258  1259.278983   \n",
       "307         \"Tea\" 2025-10-29  1214.777405     1530.822126  1240.372584   \n",
       "308         \"Tea\" 2025-10-30  1216.580007     1545.850469  1198.768587   \n",
       "309         \"Tea\" 2025-10-31  1214.906532     1489.076728  1182.720575   \n",
       "\n",
       "             Add  \n",
       "0    1178.244696  \n",
       "1    1216.963902  \n",
       "2    1343.298368  \n",
       "3    1520.046579  \n",
       "4    1516.852784  \n",
       "..           ...  \n",
       "305  1204.376328  \n",
       "306  1222.421475  \n",
       "307  1238.689590  \n",
       "308  1216.639854  \n",
       "309  1151.727614  \n",
       "\n",
       "[310 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats_model(\"non\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00ca0f",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9755f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "from window_ops.expanding import expanding_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7606700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary models from various libraries\n",
    "\n",
    "# LGBMRegressor: A gradient boosting framework that uses tree-based learning algorithms from the LightGBM library\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# XGBRegressor: A gradient boosting regressor model from the XGBoost library\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# LinearRegression: A simple linear regression model from the scikit-learn library\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec7291f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MLForecast object\n",
    "mlf = MLForecast(\n",
    "    models=[LGBMRegressor(), XGBRegressor(), LinearRegression()],  # List of models for forecasting: LightGBM, XGBoost and Linear Regression\n",
    "    freq='D',  # Frequency of the data - 'D' for daily frequency\n",
    "    lags=list(range(1, 7)),  # Specific lags to use as regressors: 1 to 6 days\n",
    "    lag_transforms = {\n",
    "        1:  [expanding_mean],  # Apply expanding mean transformation to the lag of 1 day\n",
    "    },\n",
    "    date_features=['ds'],  # Date features to use as regressors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "291cc734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2025-11-01\n",
       "1      2025-11-02\n",
       "2      2025-11-03\n",
       "3      2025-11-04\n",
       "4      2025-11-05\n",
       "          ...    \n",
       "305    2025-11-27\n",
       "306    2025-11-28\n",
       "307    2025-11-29\n",
       "308    2025-11-30\n",
       "309    2025-12-01\n",
       "Name: ds, Length: 310, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicct[\"test\"][\"non\"][\"ds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08321860",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'with_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2356\\2156386926.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# I cant do this aaaaaaaaaahhhhhhhh\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split, parts \u001b[38;5;28;01min\u001b[39;00m dicct.items():\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, df \u001b[38;5;28;01min\u001b[39;00m parts.items():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         dicct[split][key] = df.with_columns(\n\u001b[32m      7\u001b[39m             pl.col(\u001b[33m\"ds\"\u001b[39m).str.to_datetime()\n\u001b[32m      8\u001b[39m         ).sort(\u001b[33m\"ds\"\u001b[39m)\n",
      "\u001b[32mc:\\Users\\cleo7\\FHOOE\\3.S\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6318\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6319\u001b[39m         ):\n\u001b[32m   6320\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6321\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'with_columns'"
     ]
    }
   ],
   "source": [
    "# Jonas, pls do thisssss\n",
    "# I cant do this aaaaaaaaaahhhhhhhh\n",
    "\n",
    "for split, parts in dicct.items():\n",
    "    for key, df in parts.items():\n",
    "        dicct[split][key] = df.with_columns(\n",
    "            pl.col(\"ds\").str.to_datetime()\n",
    "        ).sort(\"ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18cdce0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2356\\4292244978.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_non_pd = dicct[\u001b[33m\"train\"\u001b[39m][\u001b[33m\"non\"\u001b[39m].to_pandas()\n\u001b[32m      2\u001b[39m mlf.fit(train_non_pd, prediction_intervals=PredictionIntervals(n_windows=\u001b[32m28\u001b[39m))\n",
      "\u001b[32mc:\\Users\\cleo7\\FHOOE\\3.S\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6318\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6319\u001b[39m         ):\n\u001b[32m   6320\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6321\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'to_pandas'"
     ]
    }
   ],
   "source": [
    "train_non_pd = dicct[\"train\"][\"non\"].to_pandas()\n",
    "mlf.fit(train_non_pd, prediction_intervals=PredictionIntervals(n_windows=28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe84f6",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cf240",
   "metadata": {},
   "source": [
    "## Please read (to Jonas)\n",
    "\n",
    "I am sorry for not finishing this but I am so done with the types mismatch and you are way better at this than I am. I owe you for this. I am so sorry. Also I should hydrate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d0bc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from utilsforecast.plotting import plot_series\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS, NHITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "707d6257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Seed set to 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The time column ('ds') should have either timestamps or integers, got 'object'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m models = [NBEATS(input_size=\u001b[32m2\u001b[39m * horizon, h=horizon, max_steps=\u001b[32m100\u001b[39m, enable_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m      6\u001b[39m           NHITS(input_size=\u001b[32m2\u001b[39m * horizon, h=horizon, max_steps=\u001b[32m100\u001b[39m, enable_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[32m      7\u001b[39m nf = NeuralForecast(models=models, freq=\u001b[33m'\u001b[39m\u001b[33mME\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mnf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m Y_hat_df = nf.predict()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Plot predictions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleo7\\FHOOE\\3.S\\TSA\\.venv\\Lib\\site-packages\\neuralforecast\\core.py:467\u001b[39m, in \u001b[36mNeuralForecast.fit\u001b[39m\u001b[34m(self, df, static_df, val_size, use_init_models, verbose, id_col, time_col, target_col, distributed_config, prediction_intervals)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, (pd.DataFrame, pl_DataFrame)):\n\u001b[32m    466\u001b[39m     validate_freq(df[time_col], \u001b[38;5;28mself\u001b[39m.freq)\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     \u001b[38;5;28mself\u001b[39m.dataset, \u001b[38;5;28mself\u001b[39m.uids, \u001b[38;5;28mself\u001b[39m.last_dates, \u001b[38;5;28mself\u001b[39m.ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatic_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatic_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredict_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prediction_intervals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    476\u001b[39m         \u001b[38;5;28mself\u001b[39m.prediction_intervals = prediction_intervals\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleo7\\FHOOE\\3.S\\TSA\\.venv\\Lib\\site-packages\\neuralforecast\\core.py:296\u001b[39m, in \u001b[36mNeuralForecast._prepare_fit\u001b[39m\u001b[34m(self, df, static_df, predict_only, id_col, time_col, target_col)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.target_col = target_col\n\u001b[32m    294\u001b[39m \u001b[38;5;28mself\u001b[39m._check_nan(df, static_df, id_col, time_col, target_col)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m dataset, uids, last_dates, ds = \u001b[43mTimeSeriesDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatic_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predict_only:\n\u001b[32m    304\u001b[39m     \u001b[38;5;28mself\u001b[39m._scalers_transform(dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleo7\\FHOOE\\3.S\\TSA\\.venv\\Lib\\site-packages\\neuralforecast\\tsdataset.py:360\u001b[39m, in \u001b[36mTimeSeriesDataset.from_df\u001b[39m\u001b[34m(df, static_df, id_col, time_col, target_col)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_df\u001b[39m(df, static_df=\u001b[38;5;28;01mNone\u001b[39;00m, id_col=\u001b[33m\"\u001b[39m\u001b[33munique_id\u001b[39m\u001b[33m\"\u001b[39m, time_col=\u001b[33m\"\u001b[39m\u001b[33mds\u001b[39m\u001b[33m\"\u001b[39m, target_col=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    354\u001b[39m     \u001b[38;5;66;03m# TODO: protect on equality of static_df + df indexes\u001b[39;00m\n\u001b[32m    355\u001b[39m     \u001b[38;5;66;03m# Define indices if not given and then extract static features\u001b[39;00m\n\u001b[32m    356\u001b[39m     static, static_cols = TimeSeriesDataset._extract_static_features(\n\u001b[32m    357\u001b[39m         static_df, id_col\n\u001b[32m    358\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     ids, times, data, indptr, sort_idxs = \u001b[43mufp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m     \u001b[38;5;66;03m# processor sets y as the first column\u001b[39;00m\n\u001b[32m    364\u001b[39m     temporal_cols = pd.Index(\n\u001b[32m    365\u001b[39m         [target_col]\n\u001b[32m    366\u001b[39m         + [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (id_col, time_col, target_col)]\n\u001b[32m    367\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleo7\\FHOOE\\3.S\\TSA\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:673\u001b[39m, in \u001b[36mprocess_df\u001b[39m\u001b[34m(df, id_col, time_col, target_col)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Extract components from dataframe\u001b[39;00m\n\u001b[32m    659\u001b[39m \n\u001b[32m    660\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    670\u001b[39m \u001b[33;03m          If the data is already sorted this is `None`.\u001b[39;00m\n\u001b[32m    671\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# validations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m \u001b[43mvalidate_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[38;5;66;03m# ids\u001b[39;00m\n\u001b[32m    676\u001b[39m id_counts = counts_by_id(df, id_col)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleo7\\FHOOE\\3.S\\TSA\\.venv\\Lib\\site-packages\\utilsforecast\\validation.py:107\u001b[39m, in \u001b[36mvalidate_format\u001b[39m\u001b[34m(df, id_col, time_col, target_col)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_dt_or_int(df[time_col]):\n\u001b[32m    106\u001b[39m     times_dtype = df[time_col].dtype\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    108\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe time column (\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m) should have either timestamps or integers, got \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m     )\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# target col\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: The time column ('ds') should have either timestamps or integers, got 'object'."
     ]
    }
   ],
   "source": [
    "y = dicct[\"test\"][\"non\"]\n",
    "y_val = dicct[\"val\"][\"non\"]\n",
    "# Fit and predict with NBEATS and NHITS models\n",
    "horizon = len(y)\n",
    "models = [NBEATS(input_size=2 * horizon, h=horizon, max_steps=100, enable_progress_bar=False),\n",
    "          NHITS(input_size=2 * horizon, h=horizon, max_steps=100, enable_progress_bar=False)]\n",
    "nf = NeuralForecast(models=models, freq='ME')\n",
    "nf.fit(df=y)\n",
    "Y_hat_df = nf.predict()\n",
    "\n",
    "# Plot predictions\n",
    "plot_series(y, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot(Y_df, fcst_df, max_insample_length=28 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot(Y_df, fcst_df, max_insample_length=28 * 3,\n",
    "        models=['CrostonOptimized', 'AutoNHITS', 'SeasonalNaive', 'LGBMRegressor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdaf5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "   sf = StatsForecast(models=models, freq='D', n_jobs = -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafe_upscaling_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
